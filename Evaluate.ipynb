{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import math\r\n",
    "import spacy\r\n",
    "import torch\r\n",
    "from model import *\r\n",
    "from train import evaluate\r\n",
    "from PreProcess import *\r\n",
    "\r\n",
    "\r\n",
    "def load_best_model (device, model_path = './model.pth', store_dict = \"./data\"):\r\n",
    "    article = Field(tokenize='spacy', init_token='<sos>', eos_token='<eos>', lower=True, tokenizer_language='en_core_web_trf' ,include_lengths=True)\r\n",
    "    summary = Field(tokenize='spacy',init_token='<sos>',eos_token='<eos>',lower=True, tokenizer_language='en_core_web_trf')\r\n",
    "\r\n",
    "    train_data, valid_data, test_data = TabularDataset.splits(path=store_dict, train='train.csv', validation='val.csv', test='test.csv', format='csv', fields=[(\"text\",article),('headline',summary)])\r\n",
    "\r\n",
    "    article.build_vocab(train_data, min_freq=2)\r\n",
    "    summary.build_vocab(train_data, min_freq=2)\r\n",
    "\r\n",
    "    _, _, test_loader = BucketIterator.splits((train_data, valid_data, test_data), batch_size=64, sort_within_batch=True, sort_key = lambda x:len(x.text), device=device)\r\n",
    "\r\n",
    "    attention_layer = Attention(enc_hid_dim = 512, dec_hid_dim = 512)\r\n",
    "    encode_layer = Encoder(vocab=len(article.vocab),embeding_dim=256, encoder_hidden_dim=512, decoder_hidden_dim=512, dropout=0.5)\r\n",
    "    decode_layer = Decoder(output_dim=len(summary.vocab),emb_dim=256, enc_hid_dim=512, dec_hid_dim=512, dropout=0.5, attention=attention_layer)\r\n",
    "    model = Seq2Seq(encode_layer,decode_layer, article.vocab.stoi[article.pad_token], device).to(device)\r\n",
    "\r\n",
    "    sum_pad_ids = summary.vocab.stoi[summary.pad_token]\r\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = sum_pad_ids)\r\n",
    "\r\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\r\n",
    "    test_loss = evaluate(model, test_loader, criterion)\r\n",
    "    print(\"Best Model Info:\")\r\n",
    "    print(f'Test Loss: {test_loss:.3f} / Test PPL: {math.exp(test_loss):7.3f}')\r\n",
    "\r\n",
    "    return article,summary,model\r\n",
    "\r\n",
    "\r\n",
    "def predict(sentence, src_field, trg_field, model, device, max_len = 50):\r\n",
    "    model.eval()\r\n",
    "    if sentence is str:\r\n",
    "        nlp = spacy.load('en_core_web_trf')\r\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\r\n",
    "    else:\r\n",
    "        tokens = [token.lower() for token in sentence]\r\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]        \r\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\r\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\r\n",
    "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\r\n",
    "    with torch.no_grad():\r\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor, src_len.cpu())\r\n",
    "    mask = model.create_mask(src_tensor)        \r\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\r\n",
    "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\r\n",
    "    for i in range(max_len):\r\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\r\n",
    "        with torch.no_grad():\r\n",
    "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\r\n",
    "        attentions[i] = attention            \r\n",
    "        pred_token = output.argmax(1).item()        \r\n",
    "        trg_indexes.append(pred_token)\r\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\r\n",
    "            break\r\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\r\n",
    "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# 各项参数设置=======================================================\r\n",
    "\r\n",
    "# 待测文本\r\n",
    "text = \"The UK government has announced that a legislation to make high-speed broadband a legal right is expected to pass in early 2018. The government also said that broadband internet will be provided to everyone in the UK with at least 10 Mbps speed by 2020. The move is part of the Digital Economy Act, passed earlier this year.\"\r\n",
    "model_path = './model.pth'\r\n",
    "\r\n",
    "# ================================================================="
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "print(\"Evaluating using Device: \", device)\r\n",
    "\r\n",
    "article,summary,model = load_best_model(device,model_path = model_path ,store_dict=\"./data\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluating using Device:  cuda\n",
      "Best Model Info:\n",
      "Test Loss: 3.158 / Test PPL:  23.516\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stopwords\r\n",
    "from string import punctuation as punctuations\r\n",
    "\r\n",
    "def spacy_tokenizer(sentence):\r\n",
    "    parser = spacy.load('en_core_web_trf')\r\n",
    "    tokens = parser(sentence)\r\n",
    "    tokens = [tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_ for tok in tokens]\r\n",
    "    tokens = [tok for tok in tokens if (tok not in stopwords and tok not in punctuations)]\r\n",
    "    return tokens\r\n",
    "\r\n",
    "text_tokenized = spacy_tokenizer(text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "prediction, _ = predict(text_tokenized, article, summary, model, device)\r\n",
    "print(\"[Original Text]\")\r\n",
    "print(text)\r\n",
    "print(\"[Predicted Text]\")\r\n",
    "print(f'{\" \".join(word for word in prediction if word != \"<eos>\")}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Original Text]\n",
      "The UK government has announced that a legislation to make high-speed broadband a legal right is expected to pass in early 2018. The government also said that broadband internet will be provided to everyone in the UK with at least 10 Mbps speed by 2020. The move is part of the Digital Economy Act, passed earlier this year.\n",
      "[Predicted Text]\n",
      "uk govt high 10 lakh 10 through 10 year yr\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}